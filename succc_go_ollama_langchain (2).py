# -*- coding: utf-8 -*-
"""succc_go_ollama_langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AI5qpaNxDIFh1psoFQoST8iowcLbJdcF
"""







"""https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/"""

! rm -rf /usr/local/go && tar -C /usr/local -xzf go1.24.3.linux-amd64.tar.gz

!export PATH=$PATH:/usr/local/go/bin

! go version



! rm -rf /usr/local/go && tar -C /usr/local -xzf go1.24.3.linux-amd64.tar.gz
!export PATH=$PATH:/usr/local/go/bin

!wget https://go.dev/dl/go1.24.3.src.tar.gz

!tar -C /usr/local -xzf go1.24.3.linux-amd64.tar.gz
!export PATH=$PATH:/usr/local/go/bin

!wget https://go.dev/dl/go1.24.3.src.tar.gz

!wget https://go.dev/dl/go1.24.3.linux-amd64.tar.gz



!rm /content/go1.24.3.src.tar.gz













"""############################################

### شغال
"""

!tar -C /usr/local -xzf /content/go1.24.3.linux-amd64.tar.gz
!export PATH=$PATH:/usr/local/go/bin

package main

import "fmt"

func main() {
    fmt.Println("Hello, world!")
}

!export PATH=$PATH:/usr/local/go/bin

!export PATH=$PATH:/usr/local/go/bin



!sudo ln -s /usr/local/go/bin/go /usr/bin/go

!go version

!go run /content/1.go

"""################################################

https://go.dev/play/
"""



!git clone https://github.com/tmc/langchaingo.git



"""https://pkg.go.dev/github.com/tmc/langchaingo"""

# Commented out IPython magic to ensure Python compatibility.
# %cd langchaingo

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/langchaingo/examples/huggingface-llm-example

!go run huggingface_example.go



import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = ""

!go run /content/langchaingo/examples/huggingface-llm-example/huggingface_example.go

from langchain import HuggingFaceHub

# Print out the values to make sure they are accurate
print(f"Model: {os.environ.get('XXXXXXXXXXXXXXXXXXXXX')}")

# Initialize LLM
llm = HuggingFaceHub(repo_id="google/flan-t5-xl",
                    model_kwargs={"temperature":0.9, "max_length":512},
                    huggingfacehub_api_token=os.environ.get('HUGGINGFACEHUB_API_TOKEN')
                   )

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/langchaingo

!make



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/langchaingo/examples/huggingface-llm-example
!go run huggingface_example.go



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/langchaingo
!make

!go test $(go list ./... | grep -v /mongo)

package main

import (
	"bufio"
	"bytes"
	"encoding/json"
	"fmt"
	"io/ioutil"
	"log"
	"net/http"
	"os"
	"strings"
)

// يمكنك تغيير هذا إلى أي نموذج تريده من Hugging Face Hub
// تأكد من أن النموذج يدعم text-generation أو conversational inference API
const defaultModelID = "mistralai/Mistral-7B-Instruct-v0.1"

// بنية الطلب إلى API
type APIRequest struct {
	Inputs     string            `json:"inputs"`
	Parameters map[string]interface{} `json:"parameters,omitempty"` // معلمات إضافية اختيارية
	Options    map[string]interface{} `json:"options,omitempty"`    // خيارات مثل wait_for_model
}

// بنية الاستجابة من API (الشائعة لنماذج text-generation)
type APIResponse struct {
	GeneratedText string `json:"generated_text"`
	Error         string `json:"error,omitempty"` // في حال وجود خطأ من API
	// قد تحتوي بعض النماذج على حقول أخرى، هذه هي الأكثر شيوعًا
}

func main() {
	apiToken := os.Getenv("HF_API_TOKEN")
	if apiToken == "" {
		log.Fatal("الرجاء تعيين متغير البيئة HF_API_TOKEN بالتوكن الخاص بك من Hugging Face.")
	}

	modelID := os.Getenv("HF_MODEL_ID")
	if modelID == "" {
		modelID = defaultModelID
		fmt.Printf("لم يتم تحديد HF_MODEL_ID، سيتم استخدام النموذج الافتراضي: %s\n", modelID)
	}

	apiURL := fmt.Sprintf("https://api-inference.huggingface.co/models/%s", modelID)

	fmt.Printf("مرحباً بك في الشات مع نموذج %s!\n", modelID)
	fmt.Println("اكتب 'خروج' لإنهاء البرنامج.")
	fmt.Println("------------------------------------")

	reader := bufio.NewReader(os.Stdin)
	conversationHistory := "" // لتخزين سياق المحادثة البسيط

	for {
		fmt.Print("أنت: ")
		userInput, err := reader.ReadString('\n')
		if err != nil {
			log.Printf("خطأ في قراءة الإدخال: %v\n", err)
			continue
		}
		userInput = strings.TrimSpace(userInput)

		if strings.ToLower(userInput) == "خروج" {
			fmt.Println("وداعاً!")
			break
		}

		if userInput == "" {
			continue
		}

		// إضافة الإدخال الحالي إلى سياق المحادثة (طريقة بسيطة)
		// بعض النماذج مثل Mistral Instruct تتوقع تنسيقًا خاصًا للمحادثة
		// [INST] User Message [/INST] Model Response</s>
		// سنقوم ببناء prompt يتضمن آخر مدخل للمستخدم
		// يمكنك تحسين هذه الجزئية لتضمين تاريخ محادثة أطول وأكثر تعقيدًا

		// طريقة بسيطة لإضافة سياق، يمكن تحسينها
		// إذا كنت تستخدم نموذجًا مثل Mistral Instruct، قد تحتاج لتنسيق المدخلات بشكل مختلف
		// مثلاً: conversationHistory += fmt.Sprintf("[INST] %s [/INST]", userInput)
		// حاليًا، سنرسل فقط آخر رسالة للمستخدم
		// إذا كنت تريد سياقًا، ستحتاج إلى تعديل `prompt`
		prompt := userInput
		if conversationHistory != "" {
			// بالنسبة لنماذج مثل Mistral Instruct، قد تحتاج إلى بناء السلسلة هكذا:
			// prompt = conversationHistory + " [INST] " + userInput + " [/INST]"
			// للتسهيل، سنرسل فقط آخر رسالة ونعتمد على النموذج لفهم السياق القصير
			// أو يمكنك إرسال المحادثة بأكملها إذا كان النموذج يدعم ذلك بشكل جيد
			// prompt = conversationHistory + "\nأنت: " + userInput + "\nالنموذج:" // مثال بسيط
		}


		// بناء الطلب
		payload := APIRequest{
			Inputs: prompt,
			Parameters: map[string]interface{}{
				"max_new_tokens":      250,   // الحد الأقصى للتوكنات الجديدة التي سيتم إنشاؤها
				"return_full_text":    false, // للحصول على النص المُنشأ فقط، وليس المدخلات الأصلية
				"temperature":         0.7,   // لزيادة/تقليل عشوائية الإجابة
				// "do_sample":        true, // مهم لـ temperature أن يكون له تأثير
			},
			Options: map[string]interface{}{
				"wait_for_model": true, // الانتظار حتى يتم تحميل النموذج إذا لم يكن جاهزًا
			},
		}

		jsonData, err := json.Marshal(payload)
		if err != nil {
			log.Printf("خطأ في تحويل الطلب إلى JSON: %v\n", err)
			continue
		}

		req, err := http.NewRequest("POST", apiURL, bytes.NewBuffer(jsonData))
		if err != nil {
			log.Printf("خطأ في إنشاء الطلب: %v\n", err)
			continue
		}

		req.Header.Set("Authorization", "Bearer "+apiToken)
		req.Header.Set("Content-Type", "application/json")

		client := &http.Client{}
		resp, err := client.Do(req)
		if err != nil {
			log.Printf("خطأ في إرسال الطلب إلى API: %v\n", err)
			continue
		}
		defer resp.Body.Close()

		body, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Printf("خطأ في قراءة الاستجابة: %v\n", err)
			continue
		}

		if resp.StatusCode != http.StatusOK {
			log.Printf("خطأ من API (الحالة: %d): %s\n", resp.StatusCode, string(body))
			// محاولة تحليل رسالة الخطأ من Hugging Face إذا كانت موجودة
			var errorResponse APIResponse
			if json.Unmarshal(body, &errorResponse) == nil && errorResponse.Error != "" {
				fmt.Printf("النموذج (%s): خطأ: %s\n", modelID, errorResponse.Error)
			} else if json.Unmarshal(body, &errorResponse) == nil && len(errorResponse.GeneratedText) > 0 { // أحياناً يرجع 503 مع استجابة جزئية
				fmt.Printf("النموذج (%s) (قد يكون هناك خطأ تحميل): %s\n", modelID, errorResponse.GeneratedText)
			}
			continue
		}

		// الاستجابة الناجحة تكون عادةً مصفوفة من كائن واحد
		var apiResponses []APIResponse
		err = json.Unmarshal(body, &apiResponses)
		if err != nil {
			// محاولة تحليلها ككائن واحد (بعض النماذج قد ترجع كائنًا وليس مصفوفة)
			var singleResponse APIResponse
			if errSingle := json.Unmarshal(body, &singleResponse); errSingle == nil && singleResponse.GeneratedText != "" {
				apiResponses = []APIResponse{singleResponse}
			} else {
				log.Printf("خطأ في تحليل استجابة JSON: %v\n", err)
				log.Printf("الاستجابة المستلمة: %s\n", string(body))
				continue
			}
		}


		if len(apiResponses) > 0 && apiResponses[0].GeneratedText != "" {
			modelResponse := strings.TrimSpace(apiResponses[0].GeneratedText)
			fmt.Printf("النموذج (%s): %s\n", modelID, modelResponse)

			// تحديث سجل المحادثة (طريقة بسيطة)
			// لـ Mistral Instruct:
			// conversationHistory += fmt.Sprintf("[INST] %s [/INST] %s</s> ", userInput, modelResponse)
			// أو بطريقة أبسط:
			// conversationHistory += "\nأنت: " + userInput + "\nالنموذج: " + modelResponse
		} else if apiResponses[0].Error != "" {
			fmt.Printf("النموذج (%s): خطأ: %s\n", modelID, apiResponses[0].Error)
		} else {
			fmt.Println("لم يتم استلام نص مُنشأ من النموذج.")
			fmt.Printf("الاستجابة الكاملة: %s\n", string(body))
		}
		fmt.Println("------------------------------------")
	}
}

!export HF_API_TOKEN="XXXXXXXXXXXXXXXXXXXXX"
# اختياري: لتغيير النموذج
# export HF_MODEL_ID="EleutherAI/gpt-neo-125M"

!go run /content/2.go

# أولاً، تأكد أن ملف chat.go موجود. يمكنك إنشاؤه باستخدام %%writefile
# مثال:
# %%writefile chat.go
# package main
# // ... باقي كود الـ Go الخاص بك ...

# ثم في خلية أخرى، أو بعد خلية %%writefile:
!HF_API_TOKEN="hf_YOUR_HUGGINGFACE_API_TOKEN" go run chat.go

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!export HF_API_TOKEN="XXXXXXXXXXXXXXXXXXXXX" go run 2.go

!HF_API_TOKEN="XXXXXXXXXXXXXXXXXXXXX" go run 2.go

!HF_API_TOKEN="XXXXXXXXXXXXXXXXXXXXX" go run 2.go

!curl https://api-inference.huggingface.co/models/gpt2 \
    -X POST \
    -d '{"inputs": "Once upon a time"}' \
    -H "Authorization: Bearer XXXXXXXXXXXXXXXXXXXXX" \
    -H "Content-Type: application/json"

!curl https://api-inference.huggingface.co/models/google/gemma-2b-it \
    -X POST \
    -d '{"inputs": "Who is Napoleon Bonaparte?"}' \
    -H "Authorization: Bearer XXXXXXXXXXXXXXXXXXXXX" \
    -H "Content-Type: application/json"

!curl https://api-inference.huggingface.co/models/google/gemma-2b-it \
    -X POST \
    -d '{"inputs": "Who is Napoleon Bonaparte?"}' \
    -H "Authorization: Bearer XXXXXXXXXXXXXXXXXXXXX" \
    -H "Content-Type: application/json"


    Not Found



from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="novita",
    api_key="XXXXXXXXXXXXXXXXXXXXX",
)

completion = client.chat.completions.create(
    model="Qwen/Qwen3-30B-A3B",
    messages=[
        {
            "role": "user",
            "content": "What is the capital of France?"
        }
    ],
)

print(completion.choices[0].message)

"""### شغال#########################################"""

import google.generativeai as genai
import os

# من الأفضل تعيين المفتاح كمتغير بيئة
# os.environ['GOOGLE_API_KEY'] = "AIzaSyBOVHQZwO3LiB8juCSn1LqDyCWbYWMXQkk"
# genai.configure(api_key=os.environ['GOOGLE_API_KEY'])

# أو تمريره مباشرة (أقل أمانًا إذا كنت ستشارك الكود)
try:
    genai.configure(api_key="AIzaSyBOVHQZwO3LiB8juCSn1LqDyCWbYWMXQkk")
except Exception as e:
    print(f"Error configuring API key: {e}")
    exit()

# اختر نموذجًا مناسبًا
# للاستخدامات العامة والمحادثة، "gemini-1.5-flash-latest" أو "gemini-1.5-pro-latest" خيارات جيدة
model_name = "gemini-1.5-flash-latest" # أو "gemini-1.5-pro-latest"

try:
    model = genai.GenerativeModel(model_name)

    # للمحادثة
    chat = model.start_chat(history=[])
    prompt = "ما هي عاصمة فرنسا؟"
    print(f"أنت: {prompt}")
    response = chat.send_message(prompt)
    print(f"Gemini ({model_name}): {response.text}")

    prompt2 = "وما هي عملتها الرسمية؟"
    print(f"أنت: {prompt2}")
    response2 = chat.send_message(prompt2)
    print(f"Gemini ({model_name}): {response2.text}")

    # لتوليد نص بسيط (بدون سياق محادثة)
    # prompt_single = "اكتب قصيدة قصيرة عن الربيع."
    # response_single = model.generate_content(prompt_single)
    # print(f"\nGemini ({model_name}) - نص مُنشأ:\n{response_single.text}")

except Exception as e:
    print(f"An error occurred: {e}")
    if "API_KEY_INVALID" in str(e) or "PERMISSION_DENIED" in str(e):
        print("Please ensure your API key is correct and has the necessary permissions.")
    elif "billing" in str(e).lower():
        print("Please ensure billing is enabled for your Google Cloud project associated with this API key.")

"""##############################################"""









"""### شغال########################################"""

!go mod init gemini_chat_example # (إذا لم يكن لديك ملف go.mod بالفعل)
!go get github.com/google/generative-ai-go/genai
!go run gemini_chat.go

!go get -u github.com/google/generative-ai-go/genai

/content/gemini_chat.go
package main

import (
	"bufio" // تمت إضافة هذا
	"context"
	"fmt"
	"log"
	"os" // تمت إعادة هذا وإزالة التعليق من استخدامه لقراءة المدخلات
	"strings" // تمت إضافة هذا

	"github.com/google/generative-ai-go/genai"
	"google.golang.org/api/option"
)

func main() {
	// apiKey := "AIzaSyBOVHQZwO3LiB8juCSn1LqDyCWbYWMXQkk" // يمكنك ترك هذا أو قراءته من متغير بيئة
	apiKey := os.Getenv("GOOGLE_API_KEY") // نفضل القراءة من متغير بيئة
	if apiKey == "" {
		// إذا كنت تريد استخدام المفتاح المدمج كاحتياطي:
		apiKey = "AIzaSyBOVHQZwO3LiB8juCSn1LqDyCWbYWMXQkk"
		// log.Fatal("GOOGLE_API_KEY environment variable not set, and no fallback key provided.")
		fmt.Println("Warning: GOOGLE_API_KEY not set, using hardcoded key (not recommended for production).")
	}


	ctx := context.Background()
	client, err := genai.NewClient(ctx, option.WithAPIKey(apiKey))
	if err != nil {
		log.Fatalf("Failed to create client: %v", err)
	}
	defer client.Close()

	modelName := "gemini-1.5-flash-latest"
	model := client.GenerativeModel(modelName)
	model.SafetySettings = []*genai.SafetySetting{ // مثال لضبط إعدادات الأمان (اختياري)
		{
			Category:  genai.HarmCategoryHarassment,
			Threshold: genai.HarmBlockNone,
		},
		{
			Category:  genai.HarmCategoryHateSpeech,
			Threshold: genai.HarmBlockNone,
		},
		{
			Category:  genai.HarmCategorySexuallyExplicit,
			Threshold: genai.HarmBlockNone,
		},
		{
			Category:  genai.HarmCategoryDangerousContent,
			Threshold: genai.HarmBlockNone,
		},
	}


	fmt.Printf("Chatting with Gemini (%s). Type 'خروج' to quit.\n", modelName)
	fmt.Println("------------------------------------")

	cs := model.StartChat()
	// يمكنك تحميل سجل محادثة مبدئي إذا أردت
	// cs.History = []*genai.Content{
	// 	{Parts: []genai.Part{genai.Text("You are a helpful assistant.")}, Role: "user"},
	//  {Parts: []genai.Part{genai.Text("Okay, I will be a helpful assistant.")}, Role: "model"},
	// }


	reader := bufio.NewReader(os.Stdin) // لاستقبال مدخلات المستخدم

	for {
		fmt.Print("أنت: ")
		userInput, err := reader.ReadString('\n')
		if err != nil {
			log.Printf("Error reading input: %v\n", err)
			continue
		}
		userInput = strings.TrimSpace(userInput)

		if strings.ToLower(userInput) == "خروج" {
			fmt.Println("وداعاً!")
			break
		}

		if userInput == "" {
			continue
		}

		// إرسال رسالة المستخدم إلى Gemini
		res, err := cs.SendMessage(ctx, genai.Text(userInput))
		if err != nil {
			log.Printf("Error sending message: %v\n", err)
			if res != nil && len(res.Candidates) > 0 {
				candidate := res.Candidates[0]
				if candidate.FinishReason.String() != "" { // التحقق مما إذا كان سبب الإنهاء له قيمة
					log.Printf("Candidate finish reason: %s", candidate.FinishReason.String())
					if candidate.FinishReason.String() == "SAFETY" || candidate.FinishReason.String() == "BLOCKLIST" {
						log.Println("Response blocked, possibly due to safety settings or other reasons.")
					}
				} else {
					log.Println("Candidate finish reason not available.")
				}
				// يمكنك أيضًا طباعة الـ PromptFeedback إذا كان متاحًا وفيه معلومات مفيدة
                if res.PromptFeedback != nil {
                    for _, rating := range res.PromptFeedback.SafetyRatings {
                        log.Printf("Prompt Feedback Safety Rating: Category %s, Probability %s", rating.Category.String(), rating.Probability.String())
                    }
                }
			}
			continue // استمر في الحلقة للسماح بإدخال جديد
		}
		printResponse(res, modelName)
	}
}

func printResponse(resp *genai.GenerateContentResponse, modelName string) {
	if resp == nil {
		fmt.Printf("Gemini (%s): Received nil response.\n", modelName)
		fmt.Println("------------------------------------")
		return
	}
    if resp.PromptFeedback != nil && resp.PromptFeedback.BlockReason != genai.BlockReasonUnspecified {
         fmt.Printf("Gemini (%s): Prompt was blocked. Reason: %s\n", modelName, resp.PromptFeedback.BlockReason.String())
         // يمكنك طباعة المزيد من التفاصيل من resp.PromptFeedback.SafetyRatings إذا أردت
         fmt.Println("------------------------------------")
         return
    }

	if len(resp.Candidates) == 0 || resp.Candidates[0].Content == nil || len(resp.Candidates[0].Content.Parts) == 0 {
		fmt.Printf("Gemini (%s): No response or empty content received.\n", modelName)
		// تحقق مما إذا كان هناك سبب للإنهاء في المرشح الوحيد
		if len(resp.Candidates) > 0 {
			log.Printf("Candidate finish reason: %s", resp.Candidates[0].FinishReason.String())
		}
		fmt.Println("------------------------------------")
		return
	}

	// افتراض أن الرد نص بسيط في الجزء الأول
	// قد تحتاج إلى معالجة أكثر تعقيدًا إذا كنت تتوقع أنواعًا أخرى من الأجزاء (مثل FunctionCall)
	fullResponse := ""
	for _, part := range resp.Candidates[0].Content.Parts {
		if textPart, ok := part.(genai.Text); ok {
			fullResponse += string(textPart)
		}
	}

	if fullResponse != "" {
		fmt.Printf("Gemini (%s): %s\n", modelName, fullResponse)
	} else {
		fmt.Printf("Gemini (%s): Received content but could not extract text.\n", modelName)
	}
	fmt.Println("------------------------------------")
}

!export GOOGLE_API_KEY="AIzaSyBOVHQZwO3LiB8juCSn1LqDyCWbYWMXQkk"
# ثم شغل الكود
!go run gemini_chat.go

"""##########################"""















curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &

!ollama pull llama2

!ollama list

!ollama run llama2:latest

!curl http://localhost:11434/api/generate -d '{
"model": "llama2:latest",
"prompt": "very briefly, tell me the difference between a comet and a meteor",
!curl http://localhost:11434/api/generate -d '{
"model": "llama2:latest",
"prompt": "very briefly, tell me the difference between a comet and a meteor",
"stream": false
}'











"""شغال######################"""

!nohup ollama serve &

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# curl http://localhost:11434/api/generate -d '{
#   "model": "llama2:latest",
#   "prompt": "very briefly, tell me the difference between a comet and a meteor",
#   "stream": false
# }'

"""####################################################"""















"""شغال#####################################

https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/
"""

/content/3.go

package main

import (
  "context"
  "fmt"
  "log"

  "github.com/tmc/langchaingo/llms"
  "github.com/tmc/langchaingo/llms/ollama"
)

func main() {
  llm, err := ollama.New(ollama.WithModel("llama2:latest"))
  if err != nil {
    log.Fatal(err)
  }

  query := "very briefly, tell me the difference between a comet and a meteor"

  ctx := context.Background()
  completion, err := llms.GenerateFromSinglePrompt(ctx, llm, query)
  if err != nil {
    log.Fatal(err)
  }

  fmt.Println("Response:\n", completion)
}

!go mod init my-langchain-ollama-example

!go get github.com/tmc/langchaingo/llms
!go get github.com/tmc/langchaingo/llms/ollama

!go get github.com/tmc/langchaingo

!go run 3.go

"""##############################################"""















"""شغال#######################################

مشابه curl
"""

/content/4.go
package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io/ioutil"
	"log"
	"net/http"
	"time"
)

// بنية الطلب إلى Ollama
type OllamaRequest struct {
	Model  string `json:"model"`
	Prompt string `json:"prompt"`
	Stream bool   `json:"stream"`
	// يمكنك إضافة حقل context إذا كنت تريد متابعة محادثة
	// Context []int `json:"context,omitempty"`
}

// بنية الاستجابة من Ollama (للطلبات غير المتدفقة)
type OllamaResponse struct {
	Model           string    `json:"model"`
	CreatedAt       time.Time `json:"created_at"`
	Response        string    `json:"response"`
	Done            bool      `json:"done"`
	DoneReason      string    `json:"done_reason"`
	Context         []int     `json:"context"`
	TotalDuration   int64     `json:"total_duration"`
	LoadDuration    int64     `json:"load_duration"`
	PromptEvalCount int       `json:"prompt_eval_count"`
	PromptEvalDuration int64  `json:"prompt_eval_duration"`
	EvalCount       int       `json:"eval_count"`
	EvalDuration    int64     `json:"eval_duration"`
}

func main() {
	ollamaURL := "http://localhost:11434/api/generate"

	// قم بتغيير النموذج إذا أردت نموذجًا أسرع
	// modelToUse := "mistral:7b" // مثال: تأكد من أنك قمت بتنزيله: ollama pull mistral:7b
	modelToUse := "llama2:latest" // أو استمر في استخدام llama2

	requestPayload := OllamaRequest{
		Model:  modelToUse,
		Prompt: "very briefly, tell me the difference between a comet and a meteor",
		Stream: false,
	}

	jsonData, err := json.Marshal(requestPayload)
	if err != nil {
		log.Fatalf("Error marshalling JSON: %v", err)
	}

	req, err := http.NewRequest("POST", ollamaURL, bytes.NewBuffer(jsonData))
	if err != nil {
		log.Fatalf("Error creating request: %v", err)
	}
	req.Header.Set("Content-Type", "application/json")

	client := &http.Client{
		Timeout: 10 * time.Minute, // تعيين مهلة أطول للنماذج الكبيرة
	}

	fmt.Printf("Sending request to Ollama with model %s...\n", modelToUse)
	resp, err := client.Do(req)
	if err != nil {
		log.Fatalf("Error sending request to Ollama: %v", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		bodyBytes, _ := ioutil.ReadAll(resp.Body)
		log.Fatalf("Ollama returned non-OK status: %s, Body: %s", resp.Status, string(bodyBytes))
	}

	body, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		log.Fatalf("Error reading response body: %v", err)
	}

	var ollamaResp OllamaResponse
	err = json.Unmarshal(body, &ollamaResp)
	if err != nil {
		log.Fatalf("Error unmarshalling Ollama response: %v\nResponse Body: %s", err, string(body))
	}

	fmt.Println("\n--- Ollama Response ---")
	fmt.Printf("Model: %s\n", ollamaResp.Model)
	fmt.Printf("Response:\n%s\n", ollamaResp.Response)
	fmt.Printf("Done: %t (Reason: %s)\n", ollamaResp.Done, ollamaResp.DoneReason)
	fmt.Printf("Context length: %d tokens\n", len(ollamaResp.Context))
	fmt.Printf("Total duration: %.2f seconds\n", float64(ollamaResp.TotalDuration)/1e9)
	fmt.Printf("Load duration: %.2f seconds\n", float64(ollamaResp.LoadDuration)/1e9)
	fmt.Println("-----------------------")

	// الآن يمكنك استخدام ollamaResp.Context لمتابعة المحادثة إذا أردت
}

!nohup ollama serve &

!go run 4.go

"""############################################"""









"""https://github.com/ollama/ollama/tree/main/api"""









!pip install ollama

!nohup ollama serve &

from ollama import Ollama

ollama = Ollama(base_url="http://localhost:11434")

response = ollama.generate(
    model="llama2:latest",
    prompt="very briefly, tell me the difference between a comet and a meteor",
    stream=False
)

print(response)

from ollama.client import Client  # Import Client from ollama.client

ollama = Client(base_url="http://localhost:11434")  # Instantiate the Client class

response = ollama.generate(
    model="llama2:latest",
    prompt="very briefly, tell me the difference between a comet and a meteor",
    stream=False
)

print(response)

!curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt":"Why is the sky blue?"
}'

!nohup ollama serve &

!ollama list

"""شغال"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# curl http://localhost:11434/api/generate -d '{
#   "model": "llama2:latest",
#   "prompt":"Why is the sky blue?"
# }'

"""https://github.com/ollama/ollama-python

شغال
"""

from ollama import chat
from ollama import ChatResponse

response: ChatResponse = chat(model='llama2:latest', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
print(response['message']['content'])
# or access fields directly from the response object
print(response.message.content)

from ollama import chat

stream = chat(
    model='llama3.2',
    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],
    stream=False,
)

for chunk in stream:
  print(chunk['message']['content'], end='', flush=True)

!ollama.chat(model='llama3.2', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])

!ollama.generate(model='llama3.2', prompt='Why is the sky blue?')

!ollama.create(model='example', from_='llama3.2', system="You are Mario from Super Mario Bros.")